[suite]
# mode: oper for the ACTUAL OPERATIONAL RUN, exp for other
#       main differences: queue
# "oper" will try to submit to a reservation owned by HPC_USER
suite_mode = exp
# wait for another suite to finish?
# NOTE: the suite MUST exist on the same server
#trigger = /alaro_double_hpc/4km/run/{cycle}/post_processing
#trigger = /alaro_4km/cycle/00/forecast
# NOTE: the current operational cycles are "cron" nodes
#    so they are never "complete". You must use a sub-node as trigger
trigger = 
realtime = no
suite_type = forecast_cycle

[platform]
platform = ecmwf_atos

ncores_forecast = 128*4
ncores_pre = 3*128
ncores_pos = 24
walltime_forecast = 15
walltime_pre = 15
walltime_pos = 5
# 903 on global data needs a lot of RAM and buffers!
# without OpenMP, you need more nodes (3 nodes crashes -> RAM?)
# so overriding the default header is more efficient
SELECT_PRE = --nodes=3 --ntasks-per-node=32 --cpus-per-task=4 --threads-per-core=1

[cycle]
# ALERT: if e.g. 21 and 00 cycles both have 4:30 as start time, 
# 00 can only start after the 21 cycle has started
# so the 00h analysis will miss the trigger!
# so we  don't put a time slot on the 00h run, only on the 21h
# and let 00h start as soon as 21h has progressed enough.
cycle_inc   = 6
forecast_length = 12 
trigger_time    = 10:30,16:30,22:30,04:30
cycle_labels    = midnight, morning, midday, evening
runcycles = 0,6,12,18

[assimilation]
assimilation = no

[model]
MODEL       = alaro
SURFACE     = isba
DOMAIN      = BE40a_l
HYDROSTATIC = yes
DFI         = yes
TIMESTEP    = 180
NLEVELS     = 87
CNMEXP      = AO40
ENV_ALADIN  = ENV_ecmwf_atos
# some namelist choices
NAMELIST_VERSION = alaro_oper
NAMELIST_FORECAST = alro_hsis_fcst_cy43.nam namfpc_inline_dummy namchk_oper namsteps_none
NAMELIST_PREP = nam.46t1.903 BE40a_l.87

[coupling]
#COUPLING    = RMI-ectrans
COUPLING    = HRES-mars-903
COUPLING_DOMAIN = FRBEe_q
LBC_INC     = 1

[settings]
HPC_HOST    = aa
# default HPC account is the ecflow user name
HPC_USER    = @ECF_USER@
SCRATCH     = /scratch/cv6
#ECF_LOGHOST = atos
#ECF_LOGPORT = 36780
#MAIL_LIST=dalex@@oma.be
# CLIM location
DPATH_CLIM  = @DATA_PATH@/clim/oper
# HPC paths: actual working path and a link from HOME
DATA_PATH = /ec/res4/hpcperm/cv6/NR/
BASEDIR =  /@SCRATCH@/NR/@SUITE@
# only for compatibility on other platforms
HPC_HOMEBASE = /home/@HPC_USER@/NRlink
#PACKDIR     = /ec/res4/hpcperm/cv6/accord/pack/rmi_43t2_bf.11_v0
PACKDIR     = /ec/res4/hpcperm/cv6/accord/pack/rmi_43t2_bf.08_v0
ECF_TRIES = 1

[local]
ARCH_PATH   = /scratch/cv6/FORECASTS

[postproc]
latlon_46l : pp_domain=BE70c_g2
latlon_87l : pp_domain=BE40a_g1 be70c_g1 be40a_g1,
be40a_l : pp_domain=be40a_l
BE40a_l : pp_domain=BE40a_l
be70c_l : pp_domain=be70c_l
BE70c_l : pp_domain=BE70c_l
#grib    : script=grib,
#          trigger=latlon_87l be40a_l BE40a_l BE70c_l,
#          domain_list = BE40a_g1:BL40g be70c_g1:BS70g be40a_g1:BS40g be40a_l:BS40l BE40a_l:BL40l BE70c_l:BL70l,
#export : script=export, trigger=grib latlon_46l be70c_l

[products]
# You may need to provide USER=, HOST= if this is not the default HPC user
# use local=yes for jobs running on the ecflow server
# there must be a corresponding .ecf file in the products directory
# TODO: needs more (AFD etc)
#       GRAB_HOST etc should be in settings_local, not here...
# to run from login node (not compute node): HOST=<>-nq
#save_archive_rmi : ARCH_ROOT=ao40_esuite, pp_name=BE40a_l,
#               ARCH_HOST=nori, HOST=@HPC_HOST@-nq
#               ARCH_PATH=/mnt/HDS_ALD_DATA/ALD_DATA/dalex/testruns/ar13_cycle
# save historical files to "recent" archive (1-month)
#save_hist :
#save_echkevo :
