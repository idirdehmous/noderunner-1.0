[suite]
# mode: oper for the ACTUAL OPERATIONAL RUN, exp for other
suite_mode = exp
trigger = "/BE_master/cycle/{cycle}/retrieve_mf"
realtime = yes
suite_type = forecast_cycle

[platform]
platform = ecmwf_atos

walltime_fc = 30
walltime_pre = 15
walltime_pos = 5
# 903 on global data needs a lot of RAM and buffers!
# without OpenMP, you need more nodes (3 nodes crashes -> RAM?)
# so overriding the default header is more efficient
SELECT_PRE = --qos=np --nodes=3 --ntasks-per-node=32 --cpus-per-task=4 --threads-per-core=1
SELECT_POS = --qos=nf --ntasks=24 --cpus-per-task=1 --threads-per-core=1 --mem-per-cpu=2G
SELECT_FC  = --qos=np --nodes=31 --ntasks-per-node=32 --cpus-per-task=4 --threads-per-core=1 --hint=nomultithread

[cycle]
# ALERT: if e.g. 21 and 00 cycles both have 4:30 as start time, 
# 00 can only start after the 21 cycle has started
# so the 00h analysis will miss the trigger!
# so we  don't put a time slot on the 00h run, only on the 21h
# and let 00h start as soon as 21h has progressed enough.
cycle_inc   = 6
forecast_length = 48 
trigger_time    = 04:30,10:30,16:30,22:30
cycle_labels    = 00,06,12,18
runcycles = 0,6,12,18

[assimilation]
assimilation = no

[model]
MODEL       = alaro
SURFACE     = isba
DOMAIN      = be13b_l
HYDROSTATIC = no
DFI         = yes
TIMESTEP    = 45
NLEVELS     = 87
CNMEXP      = AO13
ENV_ALADIN  = ENV_ecmwf_atos
# some namelist choices
NAMELIST_VERSION = alaro_tuned
NAMELIST_FC = alro_nhis_fcst_cy43.nam namfpc_inline_dummy namchk_none namsteps_oper
NAMELIST_PRE = nam.46t1.903nh_mf be13b_l.87
#NAMELIST_PREP = nam.46t1.903nh be13b_l.87
PACKDIR     = /ec/res4/hpcperm/cv6/accord/pack/rmi_43t2_bf.11_OMPIIFC2140_EC
# use faster 903 in cy46
MASTERODB_PRE = /ec/res4/hpcperm/cv6/accord/rootpack/46t1_bf.07.OMPIIFC2140_EC.x/bin/MASTERODB


[coupling]
#COUPLING    = RMI-ectrans
#COUPLING    = HRES-mars-903
#COUPLING_DOMAIN = FRBEe_q
COUPLING = ARP
CPL_TEMPLATE = /scratch/cv6/LBC/MF/RR/LBC_YYYYMMDDRR_HH
COUPLING_DOMAIN = FRBEe_q
LBC_INC     = 1

[settings]
HPC_HOST    = hpc
# default HPC account is the ecflow user name
HPC_USER    = @ECF_USER@
SCRATCH     = /scratch/cv6
#ECF_LOGHOST = atos
#ECF_LOGPORT = 36780
#MAIL_LIST=dalex@@oma.be
HPC_LOGPATH = /home/@HPC_USER@/NR_log
DATA_PATH = /ec/res4/hpcperm/cv6/NR/
# CLIM location
DPATH_CLIM  = @DATA_PATH@/clim/alaro_tuned
# HPC paths: actual working path and a link from HOME
BASEDIR =  /@SCRATCH@/NR/@SUITE@
# only for compatibility on other platforms
ECF_TRIES = 1
NPROC_IO = 128

[local]
ARCH_PATH   = /scratch/cv6/FORECASTS

[postproc]
gather_io : script=gather_io
latlon_87l : pp_domain=be13b_g1, trigger=gather_io
be13b_l : pp_domain=be13b_l, trigger=gather_io
grib : script=grib, trigger=latlon_87l be13b_l,
       domain_list = be13b_g1:BS13g be13b_l:BS13l,
#export : script=export, trigger=grib,
#         domain_list=be13b_g1 be13b_l, grib_list=BS13g BS13l
#         WALLTIME=00:30:00,
#         HOST=@HPC_HOST@-nq,
#         ARCH_PATH=@AFD_PATH@

[products]
#save_to_rmi : ARCH_ROOT=AO13, ARCH_TAG=atos_oper, pp_name=be13b_l,
#              WALLTIME=01:00:00
 
# You may need to provide USER=, HOST= if this is not the default HPC user
# use local=yes for jobs running on the ecflow server
# there must be a corresponding .ecf file in the products directory
# TODO: needs more (AFD etc)
#       GRAB_HOST etc should be in settings_local, not here...
# to run from login node (not compute node): HOST=<>-nq
#save_archive_rmi : ARCH_ROOT=ao40_esuite, pp_name=BE40a_l,
#               ARCH_HOST=nori, HOST=@HPC_HOST@-nq
#               ARCH_PATH=/mnt/HDS_ALD_DATA/ALD_DATA/dalex/testruns/ar13_cycle
# save historical files to "recent" archive (1-month)
#save_hist :
#save_echkevo :
