[suite]
# mode: oper for the ACTUAL OPERATIONAL RUN, exp for other, TC for time critical (ECMWF)
#       main differences: queue, data copy vs link
suite_mode = exp
# wait for another suite to finish?
# NOTE: the suite MUST exist on the same server
#trigger = /alaro_double_hpc/4km/run/{cycle}/post_processing
#trigger = /alaro_4km/cycle/00/forecast
# NOTE: the current operational cycles are "cron" nodes
#    so they are never "complete". You must use a sub-node as trigger
trigger = "/BE_master/cycle/{cycle}/retrieve_mf"
realtime = yes
suite_type = forecast_cycle

[platform]
platform = ecmwf_atos

walltime_fc = 15
walltime_pre = 25
walltime_pos = 5
# 903 on global data needs a lot of RAM and buffers!
# without OpenMP, you need more nodes (3 nodes crashes -> RAM?)
# so overriding the default header is more efficient
SELECT_FC = --qos=np --nodes=10 --ntasks-per-node=128 --cpus-per-task=1  --threads-per-core=1 --hint=nomultithread
#SELECT_PRE = --nodes=3 --ntasks-per-node=32 --cpus-per-task=4 --threads-per-core=1 --hint=nomulithread
SELECT_PRE = --qos=np --nodes=1 --ntasks-per-node=128 --cpus-per-task=1 --threads-per-core=1 --hint=nomulithread
SELECT_POS = --qos=nf --ntasks=24 --cpus-per-task=1 --threads-per-core=1 --mem-per-cpu=2G
SELECT_CANARI = --qos=nf --ntasks=4 --cpus-per-task=1 --threads-per-core=1 --mem-per-cpu=10G

[cycle]
# ALERT: if e.g. 21 and 00 cycles both have 4:30 as start time, 
# 00 can only start after the 21 cycle has started
# so the 00h analysis will miss the trigger!
# so we  don't put a time slot on the 00h run, only on the 21h
# and let 00h start as soon as 21h has progressed enough.
cycle_inc   = 6
forecast_length = 60 
trigger_time    = 03:40,10:40,15:40,22:40
cycle_labels    = 00,06,12,18
runcycles = 0,6,12,18

[assimilation]
assimilation = no
assim_upper = none
assim_surface = canari
obstypes_surface = synop
obs_npool = 4
coldstart = 2022091500


[model]
MODEL       = alaro
SURFACE     = isba
DOMAIN      = BE40a_l
HYDROSTATIC = yes
DFI         = yes
TIMESTEP    = 180
NLEVELS     = 87
CNMEXP      = AO40
ENV_ALADIN  = ENV_ecmwf_atos
# some namelist choices
NAMELIST_VERSION = alaro_oper
NAMELIST_FC = alro_hsis_fcst_cy43.nam namfpc_inline_dummy namchk_none namsteps_oper
#NAMELIST_PREP= alro_hsis_prep_cy43.nam BE40a_l.87
NAMELIST_PRE= nam.46t1.903_mf BE40a_l.87
PACKDIR     = /ec/res4/hpcperm/cv6/accord/pack/rmi_43t2_bf.08_OMPIIFC2140_EC
# non-standard executables
#MASTERODB_FC = /ec/res4/hpcperm/cv6/accord/pack/rmi_43t2_bf.08_OMPIIFC2140_EC/bin/MASTERODB
#MASTERODB_POS = /ec/res4/hpcperm/cv6/accord/pack/rmi_43t2_bf.08_v0/bin/MASTERODB
# use faster 903 in cy46
MASTERODB_PRE = /ec/res4/hpcperm/cv6/accord/rootpack/46t1_bf.07.OMPIIFC2140_EC.x/bin/MASTERODB

[coupling]
COUPLING    = ARP
#COUPLING    = HRES-mars-903
CPL_TEMPLATE = /scratch/cv6/LBC/MF/RR/LBC_YYYYMMDDRR_HH
COUPLING_DOMAIN = FRBEe_q
LBC_INC     = 1

[settings]
HPC_HOST    = hpc
# default HPC account is the ecflow user name
HPC_USER    = @ECF_USER@
SCRATCH     = /scratch/cv6
#ECF_LOGHOST = atos
#ECF_LOGPORT = 36780
#MAIL_LIST=dalex@@oma.be
# HPC paths
BASEDIR =  /@SCRATCH@/NR/@SUITE@
HPC_LOGPATH = /home/@HPC_USER@/NR_log
DATA_PATH = /ec/res4/hpcperm/cv6/NR/
# CLIM location
DPATH_CLIM  = @DATA_PATH@/clim/oper
ECF_TRIES = 1
NPROC_IO = 0

[local]
ARCH_PATH   = /scratch/cv6/FORECASTS

[postproc]
#gather_io : script=gather_io
BE40a_l : pp_domain=BE40a_l
latlon_46l : pp_domain=BE70c_g2
latlon_87l : pp_domain=BE40a_g1 be70c_g1 be40a_g1,
be40a_l : pp_domain=be40a_l
be70c_l : pp_domain=be70c_l
BE70c_l : pp_domain=BE70c_l
grib    : script=grib,
          trigger=latlon_87l be40a_l BE40a_l BE70c_l,
          domain_list = BE40a_g1:BL40g be70c_g1:BS70g be40a_g1:BS40g be40a_l:BS40l BE40a_l:BL40l BE70c_l:BL70l,
#export : script=export, trigger=grib latlon_46l be70c_l,
#         domain_list=BE40a_g1 be70c_g1 be40a_g1 be40a_l BE40a_l BE70c_l,
#         grib_list=BL40g BS70g BS40g BS40l BL40l BL70l,
#         WALLTIME=00:10:00

[products]
#save_to_rmi : ARCH_ROOT=AO40, ARCH_TAG=atos_oper, pp_name=BE40a_l,
#              WALLTIME=1:00:00

# You may need to provide USER=, HOST= if this is not the default HPC user
# use local=yes for jobs running on the ecflow server
# there must be a corresponding .ecf file in the products directory
# TODO: needs more (AFD etc)
#       GRAB_HOST etc should be in settings_local, not here...
# to run from login node (not compute node): HOST=<>-nq
#save_archive_rmi : ARCH_ROOT=ao40_esuite, pp_name=BE40a_l,
#               ARCH_HOST=nori, HOST=@HPC_HOST@-nq
#               ARCH_PATH=/mnt/HDS_ALD_DATA/ALD_DATA/dalex/testruns/ar13_cycle
# save historical files to "recent" archive (1-month)
#save_hist :
#save_echkevo :i
