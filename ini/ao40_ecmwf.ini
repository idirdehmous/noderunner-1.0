[suite]
# mode: oper for the ACTUAL OPERATIONAL RUN, exp for other
#       main differences: queue
# "oper" will try to submit to a reservation owned by HPC_USER
suite_mode = exp
# wait for another suite to finish?
# NOTE: the suite MUST exist on the same server
#trigger = /alaro_double_hpc/4km/run/{cycle}/post_processing
#trigger = /alaro_4km/cycle/00/forecast
# NOTE: the current operational cycles are "cron" nodes
#    so they are never "complete". You must use a sub-node as trigger
trigger = 
realtime = no
suite_type = forecast_cycle

[platform]
platform = ecmwf_atos

ncores_forecast = 128*31
ncores_pre = 3*128
ncores_pos = 24
walltime_forecast = 15
walltime_pre = 15
walltime_pos = 5
# 903 on global data needs a lot of RAM and buffers!
# without OpenMP, you need more nodes (3 nodes crashes -> RAM?)
# so overriding the default header is more efficient
SELECT_FORECAST = --ntasks=1280 --cpus-per-task=1 --threads-per-core=1 --hint=nomultithread
SELECT_PRE = --ntasks-per-node=32 --cpus-per-task=4 --threads-per-core=1
SELECT_POS = --ntasks=24 --cpus-per-task=1 --threads-per-core=1 --mem-per-cpu=2G
SELECT_CANARI = --ntasks=4 --cpus-per-task=1 --threads-per-core=1 --mem-per-cpu=10G

[cycle]
# ALERT: if e.g. 21 and 00 cycles both have 4:30 as start time, 
# 00 can only start after the 21 cycle has started
# so the 00h analysis will miss the trigger!
# so we  don't put a time slot on the 00h run, only on the 21h
# and let 00h start as soon as 21h has progressed enough.
cycle_inc   = 3
forecast_length = 60 
trigger_time    = 10:30,16:30,22:30,04:30
cycle_labels    = 00,03,06,09,12,15,18,21
runcycles = 0,6,12,18

[assimilation]
assimilation = yes
assim_upper = none
assim_surface = canari
obstypes_surface = synop
obs_npool = 4
coldstart = 2022091500


[model]
MODEL       = alaro
SURFACE     = isba
DOMAIN      = BE40a_l
HYDROSTATIC = yes
DFI         = yes
TIMESTEP    = 180
NLEVELS     = 87
CNMEXP      = AO40
ENV_ALADIN  = ENV_ecmwf_atos
# some namelist choices
NAMELIST_VERSION = alaro_oper
NAMELIST_FORECAST = alro_hsis_fcst_cy43.nam namfpc_inline_dummy namchk_none namsteps_oper
NAMELIST_PREP = nam.46t1.903nh BE40a_l.87
NAMELIST_CANARI = alro_hsis_cana_cy43.nam
PACKDIR     = /ec/res4/hpcperm/cv6/accord/pack/rmi_43t2_bf.11_OMPIIFC2104
# non-standard executables
MASTERODB_FC = /ec/res4/hpcperm/cv6/accord/pack/rmi_43t2_bf.08_v0/bin/MASTERODB
MASTERODB_PP = /ec/res4/hpcperm/cv6/accord/pack/rmi_43t2_bf.08_v0/bin/MASTERODB
BATOR = /home/cvah/pack/43t2_bf11_bator_mars/bin/BATOR

[coupling]
#COUPLING    = RMI-ectrans
#COUPLING    = HRES-mars-903
#COUPLING_DOMAIN = FRBEe_q
CPL_TEMPLATE = /scratch/cv6/LBC/YYYY/MM/DD/RR/ELSCFAO40ALBC0HH
COUPLING_DOMAIN = BE40a_l

LBC_INC     = 1

[settings]
HPC_HOST    = hpc
# default HPC account is the ecflow user name
HPC_USER    = @ECF_USER@
SCRATCH     = /scratch/cv6
#ECF_LOGHOST = atos
#ECF_LOGPORT = 36780
#MAIL_LIST=dalex@@oma.be
# CLIM location
DPATH_CLIM  = @DATA_PATH@/clim/alaro_tuned
# HPC paths: actual working path and a link from HOME
DATA_PATH = /ec/res4/hpcperm/cv6/NR/
BASEDIR =  /@SCRATCH@/NR/@SUITE@
HPC_LOGPATH = /home/@HPC_USER@/NR_log
#PACKDIR     = /ec/res4/hpcperm/cv6/accord/pack/rmi_43t2_bf.11_v0
ECF_TRIES = 1
NPROC_IO = 0

[local]
ARCH_PATH   = /scratch/cv6/FORECASTS

[postproc]
#gather_io : script=gather_io
BE40a_l : pp_domain=BE40a_l
latlon_46l : pp_domain=BE70c_g2
latlon_87l : pp_domain=BE40a_g1 be70c_g1 be40a_g1,
be40a_l : pp_domain=be40a_l
be70c_l : pp_domain=be70c_l
BE70c_l : pp_domain=BE70c_l
#grib    : script=grib,
#          trigger=latlon_87l be40a_l BE40a_l BE70c_l,
#          domain_list = BE40a_g1:BL40g be70c_g1:BS70g be40a_g1:BS40g be40a_l:BS40l BE40a_l:BL40l BE70c_l:BL70l,
#export : script=export, trigger=grib latlon_46l be70c_l

[products]
save_to_rmi : ARCH_ROOT=AO40, ARCH_TAG=atos_can, pp_name=BE40a_l,
              WALLTIME=1:00:00
# You may need to provide USER=, HOST= if this is not the default HPC user
# use local=yes for jobs running on the ecflow server
# there must be a corresponding .ecf file in the products directory
# TODO: needs more (AFD etc)
#       GRAB_HOST etc should be in settings_local, not here...
# to run from login node (not compute node): HOST=<>-nq
#save_archive_rmi : ARCH_ROOT=ao40_esuite, pp_name=BE40a_l,
#               ARCH_HOST=nori, HOST=@HPC_HOST@-nq
#               ARCH_PATH=/mnt/HDS_ALD_DATA/ALD_DATA/dalex/testruns/ar13_cycle
# save historical files to "recent" archive (1-month)
#save_hist :
#save_echkevo :i
